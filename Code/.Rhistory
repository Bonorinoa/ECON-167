main = "Homoskedastic Errors",
xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red")
plot(fitted(model_hetero), residuals(model_hetero),
main = "Heteroskedastic Errors",
xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red")
# Conduct Breusch-Pagan test for heteroskedasticity
bp_test_homo <- bptest(model_homo)
bp_test_hetero <- bptest(model_hetero)
# Function to simulate data with different DGPs
simulate_dgp <- function(n, independent = TRUE) {
if (independent) {
# Independent DGP
x <- rnorm(n)
e <- rnorm(n)
} else {
# Related DGP
common_factor <- rnorm(n)
x <- common_factor + rnorm(n)
e <- 0.5 * common_factor + rnorm(n)
}
y <- 2 + 3 * x + e
return(list(x = x, y = y, e = e))
}
# Generate data
n <- 1000
independent_data <- simulate_dgp(n, TRUE)
related_data <- simulate_dgp(n, FALSE)
# Plot relationship between X and errors
par(mfrow = c(1, 2))
plot(independent_data$x, independent_data$e,
main = "Independent DGP",
xlab = "X", ylab = "Errors")
cor_independent <- cor.test(independent_data$x, independent_data$e)
plot(related_data$x, related_data$e,
main = "Related DGP",
xlab = "X", ylab = "Errors")
cor_related <- cor.test(related_data$x, related_data$e)
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- x1 + x2  # Perfect linear combination
data_mc <- data.frame(y, x1, x2, x3)
model_mc <- lm(y ~ x1 + x2 + x3, data = data_mc)
summary(model_mc)  # Note NA for x3
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "/", ncol(X))
n <- 1000
beta0 <- 2
beta1 <- 3
z <- rnorm(n)
x1_endo <- 0.5*z + rnorm(n)  # x1 correlated with z
epsilon_endo <- 2*z + rnorm(n)
y_endo <- beta0 + beta1*x1_endo + epsilon_endo
model_endo <- lm(y_endo ~ x1_endo)
summary(model_endo)$coefficients  # Biased beta1 estimate
# Exogenous X (independent of epsilon)
x_exog <- rnorm(n)
epsilon_exog <- rnorm(n)
y_exog <- beta0 + beta1*x_exog + epsilon_exog
model_exog <- lm(y_exog ~ x_exog)
summary(model_exog)$coefficients
# Endogenous X (correlated with epsilon)
x_endo <- epsilon_exog + rnorm(n)
y_endo <- beta0 + beta1*x_endo + epsilon_exog
model_endo <- lm(y_endo ~ x_endo)
summary(model_endo)$coefficients  # Biased estimate
```{r multicollinearity}
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
# Create comparison table
se_comparison <- data.frame(
Variable = c("Intercept", "x1", "x2"),
SE_Without_Multicollinearity = se_perfect,
SE_With_Multicollinearity = se_imperfect
)
se_perfect
se_imperfect
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
# Create comparison table
se_comparison <- data.frame(
Variable = c("Intercept", "x1", "x2", "x3_imperfect"),
SE_Without_Multicollinearity = se_perfect,
SE_With_Multicollinearity = se_imperfect
)
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
# Create comparison table
se_comparison <- data.frame(
Variable = c("Intercept", "x1", "x2", "x3_imperfect"),
SE_Without_Multicollinearity = cbind(se_perfect, NA),
SE_With_Multicollinearity = se_imperfect
)
cbind(se_perfect, NA)
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
# Create comparison table
se_comparison <- data.frame(
Variable = c("Intercept", "x1", "x2", "x3_imperfect"),
SE_Without_Multicollinearity = rbind(se_perfect, NA),
SE_With_Multicollinearity = se_imperfect
)
print(se_comparison)
se_perfect
se_imperfect
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
print("Standard errors with perfect multicollinearity:")
print(se_perfect)
print("Standard errors with imperfect multicollinearity:")
print(se_imperfect)
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
print("\n\nStandard errors with perfect multicollinearity:\n")
print(se_perfect)
print("\n\nStandard errors with imperfect multicollinearity:\n")
print(se_imperfect)
# Set sample size
n <- 100
# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3
# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2
# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)
# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)
# Check model results
summary(model_mc)  # Note the NA coefficient for x3
# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")
# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)
# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]
cat("\n\nStandard errors with perfect multicollinearity:\n", se_perfect)
cat("\n\nStandard errors with imperfect multicollinearity:\n", se_imperfect)
# Set sample size and true parameters
n <- 1000
beta0 <- 1
beta1 <- 2
# Generate instrument (z) and error term with common factor
z <- rnorm(n)
epsilon <- 2*z + rnorm(n)  # Error term depends on z
# Generate x1 with endogeneity
x1_endo <- 0.5*z + rnorm(n)  # x1 correlated with z and thus with epsilon
# Generate dependent variable
y_endo <- beta0 + beta1*x1_endo + epsilon
# Generate exogenous case for comparison
x1_exog <- rnorm(n)  # Independent x1
epsilon_exog <- rnorm(n)  # Independent error
y_exog <- beta0 + beta1*x1_exog + epsilon_exog
# Fit both models
model_endo <- lm(y_endo ~ x1_endo)
model_exog <- lm(y_exog ~ x1_exog)
# Create comparison of estimates
results <- data.frame(
Parameter = c("beta0", "beta1"),
True_Value = c(beta0, beta1),
Exogenous_Estimate = coef(model_exog),
Endogenous_Estimate = coef(model_endo)
)
# Visualize bias
par(mfrow = c(1, 2))
plot(x1_exog, epsilon_exog,
main = "Exogenous Case: x ⊥ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
plot(x1_endo, epsilon,
main = "Endogenous Case: x ∼ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
print(results)
# Set sample size and true parameters
n <- 1000
beta0 <- 1
beta1 <- 2
# Generate instrument (z) and error term with common factor
z <- rnorm(n)
epsilon <- 2*z + rnorm(n)  # Error term depends on z
# Generate x1 with endogeneity
x1_endo <- 0.5*z + rnorm(n)  # x1 correlated with z and thus with epsilon
# Generate dependent variable
y_endo <- beta0 + beta1*x1_endo + epsilon
# Generate exogenous case for comparison
x1_exog <- rnorm(n)  # Independent x1
epsilon_exog <- rnorm(n)  # Independent error
y_exog <- beta0 + beta1*x1_exog + epsilon_exog
# Fit both models
model_endo <- lm(y_endo ~ x1_endo)
model_exog <- lm(y_exog ~ x1_exog)
# Create comparison of estimates
results <- data.frame(
Parameter = c("beta0", "beta1"),
True_Value = c(beta0, beta1),
Exogenous_Estimate = coef(model_exog),
Endogenous_Estimate = coef(model_endo)
)
# Visualize bias
par(mfrow = c(1, 2))
plot(x1_exog, epsilon_exog,
main = "Exogenous Case: x ⊥ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
plot(x1_endo, epsilon,
main = "Endogenous Case: x ~ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
print(results)
# Set sample size and true parameters
n <- 1000
beta0 <- 1
beta1 <- 2
# Generate instrument (z) and error term with common factor
z <- rnorm(n)
epsilon <- 2*z + rnorm(n)  # Error term depends on z
# Generate x1 with endogeneity
x1_endo <- 0.5*z + rnorm(n)  # x1 correlated with z and thus with epsilon
# Generate dependent variable
y_endo <- beta0 + beta1*x1_endo + epsilon
# Generate exogenous case for comparison
x1_exog <- rnorm(n)  # Independent x1
epsilon_exog <- rnorm(n)  # Independent error
y_exog <- beta0 + beta1*x1_exog + epsilon_exog
# Fit both models
model_endo <- lm(y_endo ~ x1_endo)
model_exog <- lm(y_exog ~ x1_exog)
# Create comparison of estimates
results <- data.frame(
Parameter = c("beta0", "beta1"),
True_Value = c(beta0, beta1),
Exogenous_Estimate = coef(model_exog),
Endogenous_Estimate = coef(model_endo)
)
# Visualize bias
par(mfrow = c(1, 2))
plot(x1_exog, epsilon_exog,
main = "Exogenous Case: x ⊥ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
plot(x1_endo, epsilon,
main = "Endogenous Case: x ~ ε",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
results
```{r spherical_disturbances}
# Generate data with homoskedastic errors
x <- runif(n, -2, 2)
epsilon_homo <- rnorm(n, 0, 1)  # Constant variance
y_homo <- beta0 + beta1*x + epsilon_homo
# Generate heteroskedastic errors where variance increases with x
epsilon_hetero <- rnorm(n, 0, 0.5 + abs(x))  # Variance depends on x
y_hetero <- beta0 + beta1*x + epsilon_hetero
# Fit models
model_homo <- lm(y_homo ~ x)
model_hetero <- lm(y_hetero ~ x)
# Create visualization
par(mfrow = c(2, 2))
# Plot original data
plot(x, y_homo,
main = "Homoskedastic Data",
xlab = "x", ylab = "y")
abline(model_homo, col = "blue")
plot(x, y_hetero,
main = "Heteroskedastic Data",
xlab = "x", ylab = "y")
abline(model_hetero, col = "blue")
# Plot residuals
plot(x, residuals(model_homo),
main = "Homoskedastic Residuals",
xlab = "x", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
plot(x, residuals(model_hetero),
main = "Heteroskedastic Residuals",
xlab = "x", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
# Perform Breusch-Pagan test
bp_homo <- bptest(model_homo)
bp_hetero <- bptest(model_hetero)
cat("\nBreusch-Pagan test p-values:\n")
cat("Homoskedastic case:", bp_homo$p.value, "\n")
cat("Heteroskedastic case:", bp_hetero$p.value, "\n")
# Function to simulate data with different DGPs
simulate_dgp <- function(n, independent = TRUE) {
if (independent) {
# Independent DGP
x <- rnorm(n)
epsilon <- rnorm(n)
} else {
# Common shock affects both x and epsilon
shock <- rnorm(n)
x <- 0.7*shock + rnorm(n)
epsilon <- 0.7*shock + rnorm(n)
}
y <- beta0 + beta1*x + epsilon
return(list(x = x, y = y, epsilon = epsilon))
}
# Generate data from both processes
n <- 1000
independent_dgp <- simulate_dgp(n, TRUE)
dependent_dgp <- simulate_dgp(n, FALSE)
# Fit models
model_independent <- lm(independent_dgp$y ~ independent_dgp$x)
model_dependent <- lm(dependent_dgp$y ~ dependent_dgp$x)
# Visualize relationships between x and epsilon
par(mfrow = c(1, 2))
plot(independent_dgp$x, independent_dgp$epsilon,
main = "Independent DGP",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
plot(dependent_dgp$x, dependent_dgp$epsilon,
main = "Dependent DGP\n(Common Shock)",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
# Compare estimates
results_dgp <- data.frame(
Parameter = c("beta0", "beta1"),
True_Value = c(beta0, beta1),
Independent_DGP = coef(model_independent),
Dependent_DGP = coef(model_dependent)
)
print(results_dgp)
# Function to simulate data with different DGPs
simulate_dgp <- function(n, independent = TRUE) {
if (independent) {
# Independent DGP
x <- rnorm(n)
epsilon <- rnorm(n)
} else {
# Common shock affects both x and epsilon
shock <- rnorm(n)
x <- 0.7*shock + rnorm(n)
epsilon <- 0.7*shock + rnorm(n)
}
y <- beta0 + beta1*x + epsilon
return(list(x = x, y = y, epsilon = epsilon))
}
# Generate data from both processes
n <- 1000
independent_dgp <- simulate_dgp(n, TRUE)
dependent_dgp <- simulate_dgp(n, FALSE)
# Fit models
model_independent <- lm(independent_dgp$y ~ independent_dgp$x)
model_dependent <- lm(dependent_dgp$y ~ dependent_dgp$x)
# Visualize relationships between x and epsilon
par(mfrow = c(1, 2))
plot(independent_dgp$x, independent_dgp$epsilon,
main = "Independent DGP",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
plot(dependent_dgp$x, dependent_dgp$epsilon,
main = "Dependent DGP\n(Common Shock)",
xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)
# estimates
cat("Independent DGP:\n", summary(model_independent), "\n")
# estimates
print(summary(model_independent))
print(summary(model_dependent))
