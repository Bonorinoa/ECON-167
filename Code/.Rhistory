# Part 5: Law of Large Numbers Demonstration ---------------------
# Demonstrate convergence of sample mean to theoretical mean
sample_sizes <- seq(10, 10000, by = 100)
sample_means <- numeric(length(sample_sizes))
for (i in seq_along(sample_sizes)) {
sample_means[i] <- mean(rnorm(sample_sizes[i], mean = 2, sd = 1.5))
}
# Plot convergence
plot(sample_sizes, sample_means, type = "l",
main = "Law of Large Numbers Demonstration",
xlab = "Sample Size", ylab = "Sample Mean",
ylim = c(1.5, 2.5))
abline(h = 2, col = "red", lty = 2)
legend("topright", legend = c("Sample Mean", "True Mean"),
col = c("black", "red"), lty = c(1, 2))
plot_data
set.seed(42)
library(ggplot2)
library(dplyr)
library(tidyr)
#========== WEAK AND STRONG LAW OF LARGE NUMBERS ==========
# Theory: As n increases, sample means converge to population mean
# WLLN: convergence in probability
# SLLN: convergence almost surely (with probability 1)
# Function to simulate means for different sample sizes
simulate_means <- function(n_samples, max_size, distribution_fn) {
means_matrix <- matrix(NA, nrow = n_samples, ncol = max_size)
for(i in 1:n_samples) {
x <- distribution_fn(max_size)
means_matrix[i,] <- cumsum(x) / 1:max_size
}
return(means_matrix)
}
# Simulate for different distributions
n_samples <- 100
max_size <- 1000
# Exponential(1): mean = 1
exp_means <- simulate_means(n_samples, max_size,
function(n) rexp(n, rate=1))
# Create plotting data
plot_data <- data.frame(
sample_size = rep(1:max_size, n_samples),
sample_mean = c(exp_means),
simulation = rep(1:n_samples, each = max_size)
)
# Visualize WLLN/SLLN
ggplot(plot_data, aes(x = sample_size, y = sample_mean, group = simulation)) +
geom_line(alpha = 0.1) +
geom_hline(yintercept = 1, color = "red", linetype = "dashed") +
scale_x_log10() +
labs(title = "Law of Large Numbers - Exponential(1)",
subtitle = "Each line represents one simulation path",
x = "Sample Size (log scale)",
y = "Sample Mean") +
theme_minimal()
plot_data
# Visualization code
plot_convergence <- function(N = 100, epsilon = 0.1) {
results <- matrix(rbinom(N * 100, 1, 0.5),
nrow = 100, ncol = N)
running_means <- t(apply(results, 1, cumsum) / 1:N)
plot(1:N, running_means[1,], type = "n",
ylim = c(0, 1),
xlab = "Number of Flips",
ylab = "Estimated Probability",
main = "Convergence of Sample Proportions")
# Add reference lines
abline(h = 0.5, col = "red", lwd = 2)
abline(h = 0.5 + epsilon, col = "gray", lty = 2)
abline(h = 0.5 - epsilon, col = "gray", lty = 2)
# Plot 100 different sequences
for(i in 1:100) {
lines(1:N, running_means[i,], col = rgb(0,0,1,0.1))
}
}
plot_convergence()
set.seed(234)
# Function to simulate coin flips and check if estimate deviates by more than epsilon
check_convergence <- function(N, epsilon = 0.1, replications = 10000) {
# Matrix to store results: each row is a replication of N coin flips
results <- matrix(rbinom(N * replications, 1, 0.5),
nrow = replications, ncol = N)
# Calculate proportion of heads for each replication
proportions <- rowMeans(results)
# Calculate probability of deviating by more than epsilon
prob_deviation <- mean(abs(proportions - 0.5) > epsilon)
return(prob_deviation)
}
# Test different sample sizes
sample_sizes <- c(10, 50, 100, 500)
results <- sapply(sample_sizes, check_convergence)
# Print results
for(i in seq_along(sample_sizes)) {
cat("N =", sample_sizes[i],
": P(|theta_hat - 0.5| > 0.1) ≈",
round(results[i], 3), "\n")
}
set.seed(42)
# Demonstrate convergence of sample mean to theoretical mean
sample_sizes <- seq(10, 10000, by = 100)
sample_means <- numeric(length(sample_sizes))
for (i in seq_along(sample_sizes)) {
sample_means[i] <- mean(rnorm(sample_sizes[i], mean = 2, sd = 1.5))
}
# Plot convergence
plot(sample_sizes, sample_means, type = "l",
main = "Law of Large Numbers Demonstration",
xlab = "Sample Size", ylab = "Sample Mean",
ylim = c(1.5, 2.5))
abline(h = 2, col = "red", lty = 2)
legend("topright", legend = c("Sample Mean", "True Mean"),
col = c("black", "red"), lty = c(1, 2))
# Theory: For large n, sample means are approximately normally distributed
# regardless of the underlying distribution
# Improved CLT demonstration
library(ggplot2)
library(gridExtra)
# Parameters
n_samples <- 10000  # Number of samples
sample_sizes <- c(1, 2, 5, 30)  # Different sample sizes to demonstrate convergence
# Generate data for different sample sizes
set.seed(42)  # For reproducibility
# Function to generate sample means and create a data frame for plotting
generate_clt_data <- function(sample_size) {
# Generate sample means from chi-square(1)
means <- replicate(n_samples, mean(rchisq(sample_size, df=1)))
data.frame(
sample_size = factor(paste("Sample Size:", sample_size)),
mean = means
)
}
# Combine all data
clt_data <- do.call(rbind, lapply(sample_sizes, generate_clt_data))
# Create the plot with smoother density estimates
ggplot(clt_data, aes(x = mean)) +
geom_density(aes(color = "Empirical"), adjust = 1.5) +
stat_function(aes(color = "Theoretical Normal"),
fun = dnorm,
args = list(
mean = 1,  # theoretical mean of chi-square(1)
sd = sqrt(2/as.numeric(gsub("Sample Size: ", "", levels(clt_data$sample_size))))
)) +
facet_wrap(~sample_size, scales = "free") +
labs(title = "Central Limit Theorem Demonstration",
subtitle = "Chi-square(1) distribution → Normal as n increases",
x = "Sample Mean",
y = "Density") +
theme_minimal() +
scale_color_manual(values = c("blue", "red"),
name = "Distribution") +
theme(legend.position = "bottom")
# Part 4: Visualization ------------------------------------------
# Create plots to visualize the distributions and relationships
par(mfrow = c(2, 2))
# Histogram of normal distribution
hist(x_normal, main = "Normal Distribution",
breaks = 30, prob = TRUE)
curve(dnorm(x, mean = 2, sd = 1.5),
add = TRUE, col = "red", lwd = 2)
# Histogram of log-normal distribution
hist(x_lognormal, main = "Log-normal Distribution",
breaks = 30, prob = TRUE)
curve(dlnorm(x, meanlog = 0, sdlog = 0.5),
add = TRUE, col = "red", lwd = 2)
# Histogram of uniform distribution
hist(x_uniform, main = "Uniform Distribution",
breaks = 30, prob = TRUE)
curve(dunif(x, min = 0, max = 5),
add = TRUE, col = "red", lwd = 2)
# Scatter plot of correlated variables
plot(x, y, main = "Correlated Variables",
xlab = "X", ylab = "Y", pch = 20, col = "blue")
# Reset plotting parameters
par(mfrow = c(1, 1))
# Part 3: Covariance and Correlation ------------------------------
# Generate correlated data
n <- 10000
rho <- 0.7  # desired correlation
# Method 1: Using mvrnorm from MASS package
library(MASS)
# 1 in the diagonal because each variable is obviously perfectly correlated with itself
# The off-diagonal elements are the correlation coefficients between the variables
sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
# mu is the true mean of each random variable
# Sigma is the variance covariance matrix
xy_data <- mvrnorm(n, mu = c(0, 0), Sigma = sigma)
x <- xy_data[, 1]
y <- xy_data[, 2]
# Compute sample covariance
# Cov(X,Y) = E[(X - μ_x)(Y - μ_y)] = ∑(x_i - x̄)(y_i - ȳ)/(n-1)
cov_xy <- cov(x, y)
# Manual computation of covariance
manual_cov_xy <- sum((x - mean(x)) * (y - mean(y))) / (length(x) - 1)
# Compute correlation
cor_xy <- cor(x, y)
cat("Covariance and Correlation:\n")
cat("Sample Covariance:", cov_xy, "\n")
cat("Manual Covariance:", manual_cov_xy, "\n")
cat("Sample Correlation:", cor_xy, "\n")
cat("True Correlation:", rho, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
var_lognormal <- var(x_lognormal)
var_uniform <- var(x_uniform)
# Manual computation to demonstrate the formula
manual_var_normal <- sum((x_normal - mean(x_normal))^2) / (length(x_normal) - 1)
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Manual Computation:", manual_var_normal, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
var_lognormal <- var(x_lognormal)
var_uniform <- var(x_uniform)
# Manual computation to demonstrate the formula
manual_var_normal <- sum((x_normal - mean(x_normal))^2)
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Manual Computation:", manual_var_normal, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
var_lognormal <- var(x_lognormal)
var_uniform <- var(x_uniform)
# Manual computation to demonstrate the formula
manual_var_normal <- sum((x_normal - mean(x_normal))^2) / (length(x_normal))
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Manual Computation:", manual_var_normal, "\n\n")
# Simulate bivariate normal data
set.seed(123)
n <- 1000
x <- rnorm(n)
y <- 0.7*x + sqrt(1-0.7^2)*rnorm(n)  # correlation ≈ 0.7
# Compare different denominators in covariance estimation
cov_n <- sum((x-mean(x))*(y-mean(y)))/n        # biased
cov_n1 <- sum((x-mean(x))*(y-mean(y)))/(n-1)   # traditional
cov_n2 <- sum((x-mean(x))*(y-mean(y)))/(n-2)   # accounting for both means
# True covariance in this case should be ≈ 0.7
print(c(cov_n, cov_n1, cov_n2))
# Part 3: Covariance and Correlation ------------------------------
# Generate correlated data
n <- 10000
rho <- 0.7  # desired correlation
# Method 1: Using mvrnorm from MASS package
library(MASS)
# 1 in the diagonal because each variable is obviously perfectly correlated with itself
# The off-diagonal elements are the correlation coefficients between the variables
sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
# mu is the true mean of each random variable
# Sigma is the variance covariance matrix
xy_data <- mvrnorm(n, mu = c(0, 0), Sigma = sigma)
x <- xy_data[, 1]
y <- xy_data[, 2]
# Compute sample covariance
# Cov(X,Y) = E[(X - μ_x)(Y - μ_y)] = ∑(x_i - x̄)(y_i - ȳ)/(n-1)
cov_xy <- cov(x, y)
# Manual computation of covariance
# Exactly what's happening behind the scenes in the cov() function
manual_cov_xy <- sum((x - mean(x)) * (y - mean(y))) / (length(x) - 1)
cat("Covariance and Correlation:\n")
cat("Sample Covariance:", cov_xy, "\n")
cat("Manual Covariance:", manual_cov_xy, "\n")
cat("True Correlation:", rho, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
var_lognormal <- var(x_lognormal)
var_uniform <- var(x_uniform)
# Manual computation to demonstrate the formula
manual_var_normal <- sum((x_normal - mean(x_normal))^2) / (length(x_normal) - 1)
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Manual Computation:", manual_var_normal, "\n\n")
# bessel's correction for degrees of freedom
x <- c(2, 4, 6, 8)
n <- length(x)
x_bar <- mean(x)
# Calculate deviations
deviations <- x - x_bar
print(deviations)
# Note: these sum to zero by construction
print(sum(deviations))
# Compare biased and unbiased variance
biased_var <- sum((x - x_bar)^2)/n
unbiased_var <- sum((x - x_bar)^2)/(n-1)
print(biased_var)
print(unbiased_var)
# bessel's correction for degrees of freedom
x <- c(2, 4, 6, 8)
n <- length(x)
x_bar <- mean(x)
# Calculate deviations
deviations <- x - x_bar
print(deviations)
# Note: these sum to zero by construction
print(sum(deviations))
# Compare biased and unbiased variance
biased_var <- sum((x - x_bar)^2)/n
unbiased_var <- sum((x - x_bar)^2)/(n-1)
r_var <- var(x)
print(biased_var)
print(unbiased_var)
print(r_var)
# Simulate population
set.seed(123)
population <- rnorm(100000, mean=10, sd=2)
true_var <- var(population)
# Take small samples and compare estimators
n <- 5
samples <- matrix(sample(population, n*1000, replace=TRUE), ncol=n)
biased_vars <- apply(samples, 1, function(x) mean((x - mean(x))^2))
unbiased_vars <- apply(samples, 1, function(x) var(x))
# Compare averages
c(mean(biased_vars), mean(unbiased_vars), true_var)
set.seed(123)
mu <- 10  # True population mean
sigma <- 2 # True population SD
# Function to compare squared deviations
compare_deviations <- function(n) {
sample <- rnorm(n, mu, sigma)
x_bar <- mean(sample)
# Squared deviations from sample mean
dev_xbar <- sum((sample - x_bar)^2)
# Squared deviations from population mean
dev_mu <- sum((sample - mu)^2)
return(c(dev_xbar, dev_mu))
}
# Run many simulations
results <- replicate(10000, compare_deviations(5))
mean_devs <- rowMeans(results)
print(paste("Average squared deviations from x̄:", mean_devs[1]))
print(paste("Average squared deviations from μ:", mean_devs[2]))
# Set seed for reproducibility
set.seed(123)
# Part 1: Expectation (Mean) ----------------------------------------
# Simulate data from different distributions
n <- 10000  # Large sample size for better approximation
# Normal distribution
x_normal <- rnorm(n, mean = 2, sd = 1.5)
# Log-normal distribution
x_lognormal <- rlnorm(n, meanlog = 0, sdlog = 0.5)
# Uniform distribution
x_uniform <- runif(n, min = 0, max = 5)
# Compute expectations (sample means)
# E(X) = ∑(x_i)/n
mean_normal <- mean(x_normal)
mean_lognormal <- mean(x_lognormal)
mean_uniform <- mean(x_uniform)
# Compare with theoretical expectations
cat("Normal Distribution:\n")
cat("Sample Mean:", mean_normal, "\n")
cat("Theoretical Mean:", 2, "\n\n")
cat("Log-normal Distribution:\n")
cat("Sample Mean:", mean_lognormal, "\n")
cat("Theoretical Mean:", exp(0 + 0.5^2/2), "\n\n")
cat("Uniform Distribution:\n")
cat("Sample Mean:", mean_uniform, "\n")
cat("Theoretical Mean:", 2.5, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
var_lognormal <- var(x_lognormal)
var_uniform <- var(x_uniform)
# Manual computation to demonstrate the formula
manual_var_normal <- sum((x_normal - mean(x_normal))^2) / (length(x_normal)) # bessel's correction
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Manual Computation:", manual_var_normal, "\n\n")
# Part 2: Variance ------------------------------------------------
# Compute sample variances
# Var(X) = E[(X - μ)²] = ∑(x_i - x̄)²/(n-1)
var_normal <- var(x_normal)
# Manual computation to demonstrate the formula
biased_var_normal <- sum((x_normal - mean(x_normal))^2) / (length(x_normal)) # bessel's correction
cat("Variance Comparisons:\n")
cat("Normal Distribution:\n")
cat("Sample Variance:", var_normal, "\n")
cat("Theoretical Variance:", 1.5^2, "\n")
cat("Non-corrected Variance:", biased_var_normal, "\n\n")
# Exercise 2
# Create three random normal distributions with mu=5 and var=4: 1 with 10 observations, 1 with 100 observations, and 1 with 1000 observations
# Compute and print the biased and unbiased variance for each distribution
x1 <- rnorm(10, mean = 5, sd = 2)
x2 <- rnorm(100, mean = 5, sd = 2)
x3 <- rnorm(1000, mean = 5, sd = 2)
biased_var_x1 <- sum((x1 - mean(x1))^2) / (length(x1))
biased_var_x2 <- sum((x2 - mean(x2))^2) / (length(x2))
biased_var_x3 <- sum((x3 - mean(x3))^2) / (length(x3))
unbiased_var_x1 <- sum((x1 - mean(x1))^2) / (length(x1) - 1)
unbiased_var_x2 <- sum((x2 - mean(x2))^2) / (length(x2) - 1)
unbiased_var_x3 <- sum((x3 - mean(x3))^2) / (length(x3) - 1)
cat("Variance Comparisons:\n")
cat("Normal Distribution with 10 observations:\n")
cat("Biased Variance:", biased_var_x1, "\n")
cat("Unbiased Variance:", unbiased_var_x1, "\n\n")
cat("Normal Distribution with 100 observations:\n")
cat("Biased Variance:", biased_var_x2, "\n")
cat("Unbiased Variance:", unbiased_var_x2, "\n\n")
cat("Normal Distribution with 1000 observations:\n")
cat("Biased Variance:", biased_var_x3, "\n")
cat("Unbiased Variance:", unbiased_var_x3, "\n\n")
# Theory: For large n, sample means are approximately normally distributed
# regardless of the underlying distribution
library(ggplot2)
library(gridExtra)
# Parameters
n_samples <- 10000  # Number of samples
sample_sizes <- c(1, 2, 5, 30)  # Different sample sizes to demonstrate convergence
# Generate data for different sample sizes
set.seed(42)  # For reproducibility
# Function to generate sample means and create a data frame for plotting
generate_clt_data <- function(sample_size) {
# Generate sample means from chi-square(1)
means <- replicate(n_samples, mean(rchisq(sample_size, df=1)))
data.frame(
sample_size = factor(paste("Sample Size:", sample_size)),
mean = means
)
}
# Combine all data
clt_data <- do.call(rbind, lapply(sample_sizes, generate_clt_data))
# Create the plot with smoother density estimates
ggplot(clt_data, aes(x = mean)) +
geom_density(aes(color = "Empirical"), adjust = 1.5) +
stat_function(aes(color = "Theoretical Normal"),
fun = dnorm,
args = list(
mean = 1,  # theoretical mean of chi-square(1)
sd = sqrt(2/as.numeric(gsub("Sample Size: ", "", levels(clt_data$sample_size))))
)) +
facet_wrap(~sample_size, scales = "free") +
labs(title = "Central Limit Theorem Demonstration",
subtitle = "Chi-square(1) distribution → Normal as n increases",
x = "Sample Mean",
y = "Density") +
theme_minimal() +
scale_color_manual(values = c("blue", "red"),
name = "Distribution") +
theme(legend.position = "bottom")
set.seed(234)
# Function to simulate coin flips and check if estimate deviates by more than epsilon
check_convergence <- function(N, epsilon = 0.1, replications = 10000) {
# Matrix to store results: each row is a replication of N coin flips
results <- matrix(rbinom(N * replications, 1, 0.5),
nrow = replications, ncol = N)
# Calculate proportion of heads for each replication
proportions <- rowMeans(results)
# Calculate probability of deviating by more than epsilon
prob_deviation <- mean(abs(proportions - 0.5) > epsilon)
return(prob_deviation)
}
# Test different sample sizes
sample_sizes <- c(10, 50, 100, 500)
results <- sapply(sample_sizes, check_convergence)
# Print results
for(i in seq_along(sample_sizes)) {
cat("N =", sample_sizes[i],
": P(|theta_hat - 0.5| > 0.1) ≈",
round(results[i], 3), "\n")
}
# Visualization code
plot_convergence <- function(N = 100, epsilon = 0.1) {
results <- matrix(rbinom(N * 100, 1, 0.5),
nrow = 100, ncol = N)
running_means <- t(apply(results, 1, cumsum) / 1:N)
plot(1:N, running_means[1,], type = "n",
ylim = c(0, 1),
xlab = "Number of Flips",
ylab = "Estimated Probability",
main = "Convergence of Sample Proportions")
# Add reference lines
abline(h = 0.5, col = "red", lwd = 2)
abline(h = 0.5 + epsilon, col = "gray", lty = 2)
abline(h = 0.5 - epsilon, col = "gray", lty = 2)
# Plot 100 different sequences
for(i in 1:100) {
lines(1:N, running_means[i,], col = rgb(0,0,1,0.1))
}
}
plot_convergence()
set.seed(42)
# Demonstrate convergence of sample mean to theoretical mean
sample_sizes <- seq(10, 10000, by = 100)
sample_means <- numeric(length(sample_sizes))
for (i in seq_along(sample_sizes)) {
sample_means[i] <- mean(rnorm(sample_sizes[i], mean = 2, sd = 1.5))
}
# Plot convergence
plot(sample_sizes, sample_means, type = "l",
main = "Law of Large Numbers Demonstration",
xlab = "Sample Size", ylab = "Sample Mean",
ylim = c(1.5, 2.5))
abline(h = 2, col = "red", lty = 2)
legend("topright", legend = c("Sample Mean", "True Mean"),
col = c("black", "red"), lty = c(1, 2))
