---
title: "ECON 167 - Linear Regression"
author: "Augusto Gonzalez-Bonorino"
date: 'Sys.Date()'
output: html_notebook
---

## Gauss-Markov Assumptions

```{r}
library(tidyverse)
library(MASS)
library(car)
library(lmtest)
set.seed(123)  # For reproducibility
```

# 1. Linearity Assumption

## Theory
The linearity assumption has two components:
1. Linearity in parameters: The relationship between X and Y can be written as a linear function
2. Linearity in disturbances: The error term enters the equation additively

Mathematically:

$$Y = X\beta + \epsilon$$

where:
- Y is the dependent variable
- X is the matrix of independent variables
- β is the vector of parameters
- ε is the vector of error terms

Let's visualize this assumption and see what happens when it's violated.

```{r linearity}
# Generate data that follows linear relationship
n <- 100
x <- runif(n, 0, 10)
epsilon <- rnorm(n, 0, 1)
y_linear <- 2 + 3 * x + epsilon

# Generate nonlinear data
y_nonlinear <- 2 + 3 * x + 0.3 * x^2 + epsilon

# Create dataframes
df_linear <- data.frame(x = x, y = y_linear)
df_nonlinear <- data.frame(x = x, y = y_nonlinear)

# Plot linear relationship
p1 <- ggplot(df_linear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  ggtitle("Linear Relationship") +
  theme_minimal()

# Plot nonlinear relationship
p2 <- ggplot(df_nonlinear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  ggtitle("Nonlinear Relationship") +
  theme_minimal()

# Display plots side by side
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


*Which of the GMAs looks immediately violated by the non linear relationship?*


```{r}
# plot residuals of previous two models side by side
model_linear <- lm(y_linear ~ x)
model_nonlinear <- lm(y_nonlinear ~ x)

# Plot residuals for linear model
df_linear$residuals <- residuals(model_linear)
p3 <- ggplot(df_linear, aes(x, residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Residuals of Linear Model") +
  theme_minimal()

# Plot residuals for nonlinear model
df_nonlinear$residuals <- residuals(model_nonlinear)
p4 <- ggplot(df_nonlinear, aes(x, residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Residuals of Nonlinear Model") +
  theme_minimal()

# Display plots side by side
gridExtra::grid.arrange(p3, p4, ncol = 2)
```



# 2. Full Rank (No Perfect Multicollinearity)

The full rank assumption requires that no independent variable can be perfectly predicted by a linear combination of other independent variables. Mathematically, this means the matrix X must have full column rank:

$rank(X) = K$ where K is the number of parameters.

Let's demonstrate the impact of multicollinearity:

```{r multicollinearity}
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- x1 + x2  # Perfect linear combination
data_mc <- data.frame(y, x1, x2, x3)
model_mc <- lm(y ~ x1 + x2 + x3, data = data_mc)
summary(model_mc)  # Note NA for x3
```

```{r}
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "/", ncol(X)) # 3 out of 4 columns are linearly independent
```

```{r multicollinearity}
# Set sample size
n <- 100

# Generate true coefficients
beta0 <- 1
beta1 <- 2
beta2 <- 3

# Generate independent variables
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create x3 as a perfect linear combination
x3 <- x1 + x2

# Generate dependent variable
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n, 0, 1)

# Fit model with perfect multicollinearity
model_mc <- lm(y ~ x1 + x2 + x3)

# Check model results
summary(model_mc)  # Note the NA coefficient for x3

# Check matrix rank
X <- model.matrix(model_mc)
cat("Matrix rank:", qr(X)$rank, "out of", ncol(X), "columns\n")

# Demonstrate imperfect multicollinearity
x3_imperfect <- 0.9*x1 + 0.9*x2 + rnorm(n, 0, 0.1)
model_imperfect <- lm(y ~ x1 + x2 + x3_imperfect)

# Compare standard errors
se_perfect <- summary(lm(y ~ x1 + x2))$coefficients[, 2]
se_imperfect <- summary(model_imperfect)$coefficients[, 2]

cat("\n\nStandard errors with perfect multicollinearity:\n", se_perfect)

cat("\n\nStandard errors with imperfect multicollinearity:\n", se_imperfect)
```


# 3. Exogeneity


The exogeneity assumption requires that the error term is uncorrelated with our independent variables. 

$$E[\epsilon|X] = 0$$

When this assumption is violated, our coefficient estimates become biased.

```{r exogeneity}
# Set sample size and true parameters
n <- 1000
beta0 <- 1
beta1 <- 2

# Generate instrument (z) and error term with common factor
z <- rnorm(n)
epsilon <- 2*z + rnorm(n)  # Error term depends on z

# Generate x1 with endogeneity
x1_endo <- 0.5*z + rnorm(n)  # x1 correlated with z and thus with epsilon

# Generate dependent variable
y_endo <- beta0 + beta1*x1_endo + epsilon

# Generate exogenous case for comparison
x1_exog <- rnorm(n)  # Independent x1
epsilon_exog <- rnorm(n)  # Independent error
y_exog <- beta0 + beta1*x1_exog + epsilon_exog

# Fit both models
model_endo <- lm(y_endo ~ x1_endo)
model_exog <- lm(y_exog ~ x1_exog)

# Create comparison of estimates
results <- data.frame(
  Parameter = c("beta0", "beta1"),
  True_Value = c(beta0, beta1),
  Exogenous_Estimate = coef(model_exog),
  Endogenous_Estimate = coef(model_endo)
)

# Visualize bias
par(mfrow = c(1, 2))
plot(x1_exog, epsilon_exog, 
     main = "Exogenous Case: x ⊥ ε",
     xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)

plot(x1_endo, epsilon, 
     main = "Endogenous Case: x ~ ε",
     xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)

```

```{r}
results
```

# 4. Spherical Disturbances

This assumption requires homoskedastic errors (constant variance) and no autocorrelation. Mathematically, this means:


$E[\epsilon\epsilon'|X] = \sigma^2I$

The term "spherical" comes from the geometric interpretation: when we plot the multivariate normal density function, the contours form perfect spheres in n-dimensional space.

Let's focus on heteroskedasticity, where error variance depends on X.

```{r spherical_disturbances}
# Generate data with homoskedastic errors
x <- runif(n, -2, 2)
epsilon_homo <- rnorm(n, 0, 1)  # Constant variance
y_homo <- beta0 + beta1*x + epsilon_homo

# Generate heteroskedastic errors where variance increases with x
epsilon_hetero <- rnorm(n, 0, 0.5 + abs(x))  # Variance depends on x
y_hetero <- beta0 + beta1*x + epsilon_hetero

# Fit models
model_homo <- lm(y_homo ~ x)
model_hetero <- lm(y_hetero ~ x)

# Create visualization
par(mfrow = c(2, 2))

# Plot original data
plot(x, y_homo, 
     main = "Homoskedastic Data",
     xlab = "x", ylab = "y")
abline(model_homo, col = "blue")

plot(x, y_hetero, 
     main = "Heteroskedastic Data",
     xlab = "x", ylab = "y")
abline(model_hetero, col = "blue")

# Plot residuals
plot(x, residuals(model_homo),
     main = "Homoskedastic Residuals",
     xlab = "x", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

plot(x, residuals(model_hetero),
     main = "Heteroskedastic Residuals",
     xlab = "x", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

# Perform Breusch-Pagan test
bp_homo <- bptest(model_homo)
bp_hetero <- bptest(model_hetero)

cat("\nBreusch-Pagan test p-values:\n")
cat("Homoskedastic case:", bp_homo$p.value, "\n")
cat("Heteroskedastic case:", bp_hetero$p.value, "\n")
```


# 5. Exogenous Data Generating Process

## Theory
This assumption requires that the process generating X is independent of the process generating the errors. While this is similar to the exogeneity assumption, it's more fundamental as it concerns the underlying data-generating process.

Let's demonstrate with a simple simulation:


```{r dgp}
# Function to simulate data with different DGPs
simulate_dgp <- function(n, independent = TRUE) {
  if (independent) {
    # Independent DGP
    x <- rnorm(n)
    epsilon <- rnorm(n)
  } else {
    # Common shock affects both x and epsilon
    shock <- rnorm(n)
    x <- 0.7*shock + rnorm(n)
    epsilon <- 0.7*shock + rnorm(n)
  }
  
  y <- beta0 + beta1*x + epsilon
  
  return(list(x = x, y = y, epsilon = epsilon))
}

# Generate data from both processes
n <- 1000
independent_dgp <- simulate_dgp(n, TRUE)
dependent_dgp <- simulate_dgp(n, FALSE)

# Fit models
model_independent <- lm(independent_dgp$y ~ independent_dgp$x)
model_dependent <- lm(dependent_dgp$y ~ dependent_dgp$x)

# Visualize relationships between x and epsilon
par(mfrow = c(1, 2))
plot(independent_dgp$x, independent_dgp$epsilon,
     main = "Independent DGP",
     xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)

plot(dependent_dgp$x, dependent_dgp$epsilon,
     main = "Dependent DGP\n(Common Shock)",
     xlab = "x", ylab = "ε")
abline(h = 0, col = "red", lty = 2)

```

```{r}
# estimates
print(summary(model_independent))

print(summary(model_dependent))
```

# Conclusion

Understanding and testing for the Gauss-Markov assumptions is crucial in regression analysis. When these assumptions hold:

1. The OLS estimator is unbiased: $E[\hat{\beta}] = \beta$
2. It has minimum variance among all linear unbiased estimators
3. The standard errors and hypothesis tests are valid

When assumptions are violated, we might need to consider:
- Transforming variables (for non-linearity)
- Using robust standard errors (for heteroskedasticity)
- Implementing instrumental variables (for endogeneity)
- Using time series methods (for autocorrelation)

### ----------------------------------------------- ###

## Linear Regression

```{r}
# simulated example
x <- rnorm(100)
y <- 2*x + rnorm(100)

regression_model <- lm(y ~ x)

plot(x, y)
abline(regression_model)
title("Simple Linear Regression Example")
mtext("y = 2x + error", side = 3, line = 0.2)
```

In the background, `lm()` is using OLS to estimate the coefficients of the model. The model is estimated as:

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line. The error term $\epsilon$ is assumed to be normally distributed with mean 0 and constant variance.

In theory, this would be the same estimates we get by solving the minimization problem in pen and paper. Recall that the OLS estimators, for univariate regression models, are:

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
$$

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}
$$

where $\bar{y}$ and $\bar{x}$ are the sample means of $y$ and $x$, respectively.

```{r}
# manually calculating the coefficients
beta_1 <- cov(x, y) / var(x)
beta_0 <- mean(y) - beta_1 * mean(x)

# lm() coefficients
coefs <- summary(regression_model)$coefficients

cat("Manually calculated coefficients:\n")
cat("Intercept: ", beta_0, "\n")
cat("Slope: ", beta_1, "\n\n")

cat("lm() coefficients:\n")
cat("Intercept: ", coefs[1, 1], "\n")
cat("Slope: ", coefs[2, 1], "\n")
```

There might be some performance difference stemming from how the `lm()` function computes the covariance and variance, specially for larger datasets or bigger models. That is, the actual algorithmic implementation might be more sophisticated than simply computing cov(x,y) and var(x) directly. Nevertheless, it is very important that you understand what is happenning under the hood. Different softwares (e.g., R packages or Stata) tend to do things differently, and it is important to understand the assumptions and limitations of the software you are using to make intelligent decisions.

## Matrix Algebra

```{r}
# Different ways to create matrices
A <- matrix(1:6, nrow = 2, ncol = 3)
B <- matrix(c(1,2,3,4), nrow = 2) # 2x2 matrix
C <- rbind(c(1,2,3), c(4,5,6))    # Binding rows
D <- cbind(c(1,2), c(3,4))        # Binding columns
I <- matrix(c(1,0,0,1), nrow = 2) # Identity matrix

# Display matrices
cat("Matrix A:\n")
print(A)
cat("\nMatrix B:\n")
print(B)
cat("\nIdentity Matrix:\n")
print(I)

# Matrix dimensions
cat("\nDimensions of A:", dim(A))
cat("\nNumber of rows:", nrow(A))
cat("\nNumber of columns:", ncol(A))
```

```{r}
# Matrix addition and subtraction
cat("B + B:\n")
print(B + B)
cat("\nB - B:\n")
print(B - B)

# Matrix multiplication
cat("\nB × B:\n")
print(B %*% B)

# Element-wise multiplication
cat("\nElement-wise multiplication of B:\n")
print(B * B)

# Transpose
cat("\nTranspose of B:\n")
print(t(B))

# Matrix inverse
## This generic function solves the equation a %*% x = b for x, where b can be either a vector or a matrix.
## In this case, we are solving B %*% x = I, where I is the identity matrix
cat("\nInverse of B:\n")
print(solve(B, I))

# Verify B × B⁻¹ = I
cat("\nB × B⁻¹ (should be identity matrix):\n")
print(round(B %*% solve(B), 10))
```

### Matrix properties

```{r}
# Determinant
det_B <- det(B)
cat("Determinant of B:", det_B)

# Eigenvalues and eigenvectors
eigen_B <- eigen(B)
cat("\n\nEigenvalues of B:\n")
print(eigen_B$values)
cat("\nEigenvectors of B:\n")
print(eigen_B$vectors)

# Condition number (useful for multicollinearity detection)
cond_num <- kappa(B)
cat("\nCondition number of B:", cond_num)

```

### Applications in Regression Analysis


```{r}
# Let's simulate data and implement OLS manually using matrix algebra.
# Set seed for reproducibility
set.seed(123)

# Generate sample data
n <- 100  # sample size
X <- cbind(1, rnorm(n), rnorm(n))  # Two regressors plus intercept
beta_true <- c(1, 2, -1)           # True parameters
epsilon <- rnorm(n, 0, 0.5)        # Error term
y <- X %*% beta_true + epsilon     # Generate dependent variable (Y = Xβ + ε)

# Manual OLS implementation using matrix algebra
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y # beta_hat = (X'X)^-1 X'y
y_hat <- X %*% beta_hat
residuals <- y - y_hat

# Compare with lm()
model_lm <- lm(y ~ X[,-1])  # exclude first column (intercept) because lm() adds it automatically

# Display results
cat("Manual OLS coefficients:\n")
print(beta_hat)
cat("\nlm() coefficients:\n")
print(coef(model_lm))

# Calculate R-squared manually
TSS <- sum((y - mean(y))^2)
RSS <- sum(residuals^2)
R_squared_manual <- 1 - RSS/TSS

cat("\nManual R-squared:", R_squared_manual)
cat("\nlm() R-squared:", summary(model_lm)$r.squared)
```

```{r}
# Create plotting data for first regressor
plot_data <- data.frame(
  x = X[,2],  # first regressor
  y = y,
  fitted = y_hat
)

# Sort for clean plotting
plot_data <- plot_data[order(plot_data$x),]

# Plot
plot(plot_data$x, plot_data$y, 
     main = "Regression Results",
     xlab = "First Regressor",
     ylab = "y",
     pch = 16,
     col = "grey50")
lines(plot_data$x, plot_data$fitted, 
      col = "red", 
      lwd = 2)
legend("topleft", 
       legend = c("Observed", "Fitted"), 
       pch = c(16, NA), 
       lty = c(NA, 1),
       col = c("grey50", "red"))
```

- There's a clear positive relationship between the first regressor and y
- The fitted line shows considerable fluctuation rather than being smooth because this is a multiple regression - what we're seeing is a 2D projection of a 3D relationship
- The scatter of points around the fitted line represents both the influence of our second regressor and the random error term
- We can only plot in 2D or 3D. In practice, with multiple regression, we need to choose one regressor to plot against. This is why we use statistical methods to estimate the relationship.
  - But you can always visualize partial regression plots to visualize the relationship between one regressor and the dependent variable, controlling for the other regressors.
- Nevertheless, since here we only have two regressors we can try a 3D plot

```{r}
if (!require("plotly")) {
    install.packages("plotly")
}
library(plotly)

# Create the surface data
x.pred <- seq(min(X[,2]), max(X[,2]), length.out = 30)
y.pred <- seq(min(X[,3]), max(X[,3]), length.out = 30)
xy <- expand.grid(x = x.pred, y = y.pred)
z.pred <- cbind(1, xy$x, xy$y) %*% beta_hat
z.pred <- matrix(z.pred, nrow = length(x.pred))

# Create the 3D plot with adjusted orientation
plot_ly() %>%
  add_surface(x = x.pred, 
             y = y.pred, 
             z = z.pred,
             opacity = 0.7,
             colorscale = list(c(0, 1), c("rgb(200,200,255)", "rgb(0,0,255)")),
             name = "Fitted Surface") %>%
  add_trace(x = X[,2],
           y = X[,3],
           z = as.vector(y),
           type = 'scatter3d',
           mode = 'markers',
           marker = list(size = 3,
                        color = 'red',
                        opacity = 0.8),
           name = "Observed Data") %>%
  layout(scene = list(
    xaxis = list(title = "First Regressor", autorange = "reversed"),
    yaxis = list(title = "Second Regressor"),
    zaxis = list(title = "y"),
    camera = list(
      eye = list(x = -1.5, y = 1.5, z = 1.5)
    )),
    title = "Interactive 3D Visualization of Multiple Regression")

```

  