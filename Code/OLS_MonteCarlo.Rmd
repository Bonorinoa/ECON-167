---
title: "OLS and Monte Carlo"
author: "Augusto Gonzalez-Bonorino"
date: "Sys.Date()"
---

# Monte Carlo Studies

Monte Carlo simulations help us see how estimators perform by simulating data many times. Let’s start with a simple example: estimating the mean of a population.

```{r}
# Set a seed for reproducibility (so results are the same each time)
set.seed(123)

# Number of simulations
n_sim <- 1000

# Sample sizes to try
sample_sizes <- c(10, 50, 100)

# True population mean and standard deviation (things we might want to estimate)
true_mean <- 5
true_sd <- 1

# List to store results
results <- list()

# Loop over each sample size
for (n in sample_sizes) {
  # For each simulation, draw n samples and compute the mean
  sample_means <- numeric(n_sim)
  for (i in 1:n_sim) {
    samples <- rnorm(n, mean = true_mean, sd = true_sd)
    sample_means[i] <- mean(samples)
  }
  results[[as.character(n)]] <- sample_means
}

# Plot histograms to visualize the sample means
par(mfrow = c(1, 3))  # Arrange plots in 1 row, 3 columns
for (n in sample_sizes) {
  hist(results[[as.character(n)]], 
       main = paste("Sample Size =", n), 
       xlab = "Sample Mean", 
       breaks = 20)
  abline(v = true_mean, col = "red", lwd = 2)  # Add true mean line
}

# Check the average and spread of sample means
for (n in sample_sizes) {
  mean_of_means <- mean(results[[as.character(n)]])
  var_of_means <- var(results[[as.character(n)]])
  theoretical_var <- (true_sd^2) / n  # Expected variance: sigma^2 / n
  cat("Sample Size:", n, "\n")
  cat("Average of Sample Means:", mean_of_means, "\n")
  cat("Variance of Sample Means:", var_of_means, "\n")
  cat("Theoretical Variance:", theoretical_var, "\n\n")
}

# we should see that the average of the sample means is close to the true population mean, and the variance of the sample means decreases as the sample size increases.
```

Average: The average of the sample means is close to 5, showing that the sample mean is unbiased.
Variance: The variance of the sample means gets smaller as sample size increases, matching the theoretical value $\sigma^2/n = 1/n$. This demonstrates how larger samples give more precise estimates

```{r}
# quick quiz: Estimate the true standard deviation of the N(5,1) sample using the unbiased estimator of the sample variance.

```


2. Monte Carlo Simulations for Regression Examples
Now, we’ll use Monte Carlo simulations to study the OLS estimator in a simple linear regression:

$$y_i = \beta_0 + \beta_1 * x_i + \epsilon_i$$

where $$ \beta_0 = 0, \, \beta_1 = 1, \, \text{and} \, x = [2, 5, 8]. $$ We’ll run two examples:

*Example 1: Homoskedastic errors* ($$ \epsilon_i \sim N(0, 1) $$)

*Example 2: Heteroskedastic errors* ($$ \epsilon_i \sim N(0, x_i^2) $$)

For each, we’ll check:

  - Unbiasedness: Is the average $ \hat{\beta}_1 $ close to the true $ \beta_1 = 1 $?

  - Standard Errors: How do the estimated standard errors from OLS compare to the actual variability?

  - Confidence Intervals: Do 95% confidence intervals cover the true $ \beta_1 $ about 95% of the time?

  - Tests: Do normality and heteroskedasticity tests behave as expected?

```{r}
library(lmtest)  # Install if needed: install.packages("lmtest")

# Function to run Monte Carlo simulation for OLS
simulate_ols <- function(n_sim, x, beta0_true, beta1_true, error_gen) {
  n <- length(x)  # Sample size
  beta1_hats <- numeric(n_sim)  # Store estimated beta1
  se_beta1 <- numeric(n_sim)    # Store estimated standard errors
  ci_lower <- numeric(n_sim)    # Store lower bounds of 95% CI
  ci_upper <- numeric(n_sim)    # Store upper bounds of 95% CI
  shapiro_p <- numeric(n_sim)   # Store p-values for normality test
  bp_p <- numeric(n_sim)        # Store p-values for heteroskedasticity test
  
  # Run simulations
  for (i in 1:n_sim) {
    # Generate errors using the provided function
    epsilon <- error_gen(x)
    # Generate y based on the true model
    y <- beta0_true + beta1_true * x + epsilon
    # Fit OLS model
    model <- lm(y ~ x)
    # Store beta1 estimate
    beta1_hats[i] <- coef(model)[2]
    # Store standard error of beta1
    se_beta1[i] <- summary(model)$coefficients[2, 2]
    # Store 95% confidence interval for beta1
    ci <- confint(model)[2, ]
    ci_lower[i] <- ci[1]
    ci_upper[i] <- ci[2]
    # Store p-values from tests
    residuals <- residuals(model)
    shapiro_p[i] <- shapiro.test(residuals)$p.value    # Normality test
    bp_p[i] <- bptest(model)$p.value                   # Heteroskedasticity test
  }
  
  # Return all results as a list
  return(list(beta1_hats = beta1_hats, 
              se_beta1 = se_beta1, 
              ci_lower = ci_lower, 
              ci_upper = ci_upper, 
              shapiro_p = shapiro_p, 
              bp_p = bp_p))
}
```

## Example 1: Homoskedastic errors 

```{r}
# Set seed for reproducibility
set.seed(123)

# Define constants
n_sim <- 10000  # Number of simulations
x <- c(2, 5, 8) # Fixed x values
beta0_true <- 0 # True beta0
beta1_true <- 1 # True beta1

# Function to generate homoskedastic errors: N(0, 1)
error_gen_homo <- function(x) {
  rnorm(length(x), mean = 0, sd = 1)
}

# Run the simulation
results_homo <- simulate_ols(n_sim, x, beta0_true, beta1_true, error_gen_homo)

# Analyze results
# Unbiasedness: average of beta1 estimates
mean_beta1_homo <- mean(results_homo$beta1_hats)
cat("Average beta1_hat:", mean_beta1_homo, "\n")

# Standard Errors: compare estimated SE to actual variability
mean_se_beta1_homo <- mean(results_homo$se_beta1)
true_se_homo <- sd(results_homo$beta1_hats)  # Actual standard error from simulation
cat("Average Estimated SE:", mean_se_beta1_homo, "\n")
cat("True SE (from simulation):", true_se_homo, "\n")

# Confidence Intervals: proportion containing true beta1
coverage_homo <- mean((results_homo$ci_lower <= beta1_true) & 
                      (results_homo$ci_upper >= beta1_true))
cat("95% CI Coverage:", coverage_homo, "\n")

# Tests: proportion of rejections at 5% level
shapiro_reject_homo <- mean(results_homo$shapiro_p < 0.05)
bp_reject_homo <- mean(results_homo$bp_p < 0.05)
cat("Normality Test Rejections:", shapiro_reject_homo, "\n")
cat("Heteroskedasticity Test Rejections:", bp_reject_homo, "\n")

# Plot histogram of beta1 estimates
hist(results_homo$beta1_hats, 
     main = "Homoskedastic Errors: beta1_hat", 
     xlab = "Estimated beta1", 
     breaks = 30)
abline(v = beta1_true, col = "red", lwd = 2)  # True beta1 line
```


Unbiasedness
The average $$\hat{\beta}_1$$ is very close to 1, confirming OLS is unbiased.

Standard Errors
The average estimated standard error matches the true standard error (standard deviation of $$\hat{\beta}_1$$), because the errors are homoskedastic and OLS assumes this correctly.

Confidence Intervals
About 95% of the 95% confidence intervals include the true $$\beta_1 = 1$$, as expected under homoskedasticity.

Normality Test
The Shapiro-Wilk test rejects normality about 5% of the time, which is correct since the errors are normal, and 5% is the false positive rate at the 0.05 level.

Heteroskedasticity Test
The Breusch-Pagan test rarely rejects, which is correct since there’s no heteroskedasticity. (Note: With only 3 observations, this test has low power, but it’s included for illustration.)

Histogram
The distribution of $$\hat{\beta}_1$$ is centered at 1 with moderate spread.

## Example 2: Heteroskedastic errors

It messes up our calculation of the variance-covariance matrix. Manageable and most often we can still trust the point estimates, but the standard errors, confidence intervals, and hypothesis tests are no longer valid. In fact, the standard errors will always be underestimated, leading to overly narrow confidence intervals and inflated test statistics. Therefore, we can use "robust" standard errors. We will see some of these techniques next week. 

```{r}
# Function to generate heteroskedastic errors: N(0, x_i^2)
error_gen_hetero <- function(x) {
  rnorm(length(x), mean = 0, sd = x)  # sd = x_i, so variance = x_i^2
}

# Run the simulation
results_hetero <- simulate_ols(n_sim, x, beta0_true, beta1_true, error_gen_hetero)

# Analyze results
# Unbiasedness: average of beta1 estimates
mean_beta1_hetero <- mean(results_hetero$beta1_hats)
cat("Average beta1_hat:", mean_beta1_hetero, "\n")

# Standard Errors: compare estimated SE to actual variability
mean_se_beta1_hetero <- mean(results_hetero$se_beta1)
true_se_hetero <- sd(results_hetero$beta1_hats)  # Actual standard error from simulation
cat("Average Estimated SE:", mean_se_beta1_hetero, "\n")
cat("True SE (from simulation):", true_se_hetero, "\n")

# Confidence Intervals: proportion containing true beta1
coverage_hetero <- mean((results_hetero$ci_lower <= beta1_true) & 
                        (results_hetero$ci_upper >= beta1_true))
cat("95% CI Coverage:", coverage_hetero, "\n")

# Tests: proportion of rejections at 5% level
shapiro_reject_hetero <- mean(results_hetero$shapiro_p < 0.05)
bp_reject_hetero <- mean(results_hetero$bp_p < 0.05)
cat("Normality Test Rejections:", shapiro_reject_hetero, "\n")
cat("Heteroskedasticity Test Rejections:", bp_reject_hetero, "\n")

# Plot histogram of beta1 estimates
hist(results_hetero$beta1_hats, 
     main = "Heteroskedastic Errors: beta1_hat", 
     xlab = "Estimated beta1", 
     breaks = 30)
abline(v = beta1_true, col = "red", lwd = 2)  # True beta1 line
```



Unbiasedness
The average $$\hat{\beta}_1$$ is still close to 1, showing OLS is unbiased even with heteroskedasticity.

Standard Errors
The average estimated standard error is much smaller than the true standard error. OLS assumes homoskedasticity, so it underestimates the variability when errors are heteroskedastic.

Confidence Intervals
The coverage is less than 95% because the standard errors are too small, making confidence intervals too narrow to capture $$\beta_1 = 1$$ reliably.

Normality Test
The Shapiro-Wilk test rejects about 5% of the time, as expected, since the errors are still normal (heteroskedasticity doesn’t affect normality).

Heteroskedasticity Test
The Breusch-Pagan test rejects more often than in Example 1, indicating it detects heteroskedasticity sometimes. However, with only 3 observations, it’s not very powerful.

Histogram
The distribution of $$\hat{\beta}_1$$ is centered at 1 but has a wider spread than in Example 1, reflecting the larger variability due to heteroskedasticity.


## Example 3: Multicollinearity

Main implication is wrong variances, hence wrong standard errors. We can correct for this using "robust" standard errors. We will see this in upcoming lectures.

We lose precision in estimating the coefficients, but the estimates are still unbiased.

```{r}
set.seed(123)
sims <- 1000
n <- 50
rho <- c(0, 0.5, 0.9)
betas <- matrix(NA, sims, length(rho))
for (j in 1:length(rho)) {
  for (i in 1:sims) {
    x1 <- runif(n, 0, 10)
    u <- rnorm(n, 0, 1)
    x2 <- rho[j] * x1 + u
    y <- 1 + 2 * x1 + 1.5 * x2 + rnorm(n)
    betas[i,j] <- coef(lm(y ~ x1 + x2))[2]  # Beta_1
  }
}
par(mfrow=c(1,3))
for (j in 1:length(rho)) {
  hist(betas[,j], main=paste("rho =", rho[j]), xlab="Beta_1")
  abline(v=2, col="red")
}
```


```{r}
set.seed(123)
n_sim <- 1000
n <- 50  # Larger sample to avoid singularity
rho_values <- c(0, 0.5, 0.9)  # Correlation levels
var_beta1 <- numeric(length(rho_values))
var_beta2 <- numeric(length(rho_values))

for (j in 1:length(rho_values)) {
  rho <- rho_values[j]
  sigma_u <- sqrt(1 - rho^2)  # Ensure Var(x2) = 1
  beta1_hats <- numeric(n_sim)
  beta2_hats <- numeric(n_sim)
  for (i in 1:n_sim) {
    x1 <- rnorm(n, 5, 1)
    u <- rnorm(n, 0, sigma_u)
    x2 <- rho * x1 + u
    epsilon <- rnorm(n, 0, 1)
    y <- 1 + 2 * x1 + 1.5 * x2 + epsilon  # True model
    model <- lm(y ~ x1 + x2)
    beta1_hats[i] <- coef(model)[2]
    beta2_hats[i] <- coef(model)[3]
  }
  var_beta1[j] <- var(beta1_hats)
  var_beta2[j] <- var(beta2_hats)
}

# Plot results
barplot(rbind(var_beta1, var_beta2), beside=TRUE, 
        names.arg=paste("rho =", rho_values), 
        legend.text=c("Var(beta1)", "Var(beta2)"),
        main="Variance of Coefficients under Multicollinearity",
        ylab="Variance")
```


## Example 4: Ommited Variable Bias

Ommiting regressors that are correlated with the included regressors can lead to biased and inconsistent estimates. Main implication is that exogeneity is broken, and hence we get highly biased estimates. 

```{r}
set.seed(123)
n_sim <- 1000
n <- 50
beta1_hats_omitted <- numeric(n_sim)
beta1_hats_full <- numeric(n_sim)

for (i in 1:n_sim) {
  x3 <- rnorm(n, 100, 10)  # Omitted variable (e.g., IQ)
  v <- rnorm(n, 0, 1)
  x1 <- 10 - 0.1 * x3 + v  # Correlated with x3
  epsilon <- rnorm(n, 0, 1)
  y <- 1 + 2 * x1 + 0.5 * x3 + epsilon  # True model
  
  # Omitted variable model
  model_omitted <- lm(y ~ x1)
  beta1_hats_omitted[i] <- coef(model_omitted)[2]
  
  # Full model
  model_full <- lm(y ~ x1 + x3)
  beta1_hats_full[i] <- coef(model_full)[2]
}

# Plot histograms
par(mfrow=c(1,2))
hist(beta1_hats_omitted, main="Omitted Variable Model", xlab="beta1_hat", breaks=20)
abline(v=2, col="red", lwd=2)
hist(beta1_hats_full, main="Full Model", xlab="beta1_hat", breaks=20)
abline(v=2, col="red", lwd=2)

# Report means
cat("Mean beta1_hat (omitted):", mean(beta1_hats_omitted), "\n")
cat("Mean beta1_hat (full):", mean(beta1_hats_full), "\n")

```



```{r}
X <- matrix(c(1,1,1,4,5,6,5.5,5,4.5), ncol=3)
solve(t(X) %*% X)
```

